# PRD-002: Database Schema & Infrastructure

**Version**: 1.0
**Date**: 2025-11-18
**Phase**: 1 - Foundation
**Duration**: 2 days
**Status**: Completed
**Dependencies**: PRD-001

---

## Objective

Design and implement the SQLite database schema that will store all collected content, analyzed data, confluence scores, and theme tracking. Establish database utilities and ORM models.

---

## Success Criteria

- [x] Complete schema designed and documented
- [x] Migration system functional
- [x] All tables created successfully
- [x] CRUD operations tested for each table
- [x] Sample data inserted and queryable
- [x] Performance acceptable (<100ms for common queries)
- [ ] Sebastian reviews and approves schema

---

## Database Schema

### Table: sources
Stores configuration for each data source.

```sql
CREATE TABLE sources (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    name TEXT NOT NULL UNIQUE,  -- "42macro", "discord_options_insight", etc.
    type TEXT NOT NULL,  -- "web", "discord", "twitter", "youtube", "rss"
    config JSON,  -- Source-specific configuration
    active BOOLEAN DEFAULT 1,
    last_collected_at TIMESTAMP,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

### Table: raw_content
Stores everything collected before AI analysis.

```sql
CREATE TABLE raw_content (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    source_id INTEGER NOT NULL,
    content_type TEXT NOT NULL,  -- "text", "pdf", "video", "image"
    content_text TEXT,  -- For text content
    file_path TEXT,  -- For files (PDFs, videos, images)
    url TEXT,  -- Original URL if applicable
    metadata JSON,  -- Author, timestamp, channel, etc.
    collected_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    processed BOOLEAN DEFAULT 0,
    FOREIGN KEY (source_id) REFERENCES sources(id)
);

CREATE INDEX idx_raw_content_source ON raw_content(source_id);
CREATE INDEX idx_raw_content_processed ON raw_content(processed);
CREATE INDEX idx_raw_content_collected_at ON raw_content(collected_at);
```

### Table: analyzed_content
Stores AI agent analysis results.

```sql
CREATE TABLE analyzed_content (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    raw_content_id INTEGER NOT NULL,
    agent_type TEXT NOT NULL,  -- "transcript", "pdf", "image", "classifier"
    analysis_result JSON NOT NULL,  -- Full structured output from agent
    key_themes TEXT,  -- Comma-separated themes for quick search
    tickers_mentioned TEXT,  -- Comma-separated tickers
    sentiment TEXT,  -- "bullish", "bearish", "neutral"
    conviction INTEGER,  -- 0-10
    time_horizon TEXT,  -- "1d", "1w", "1m", "3m", "6m+"
    analyzed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (raw_content_id) REFERENCES raw_content(id)
);

CREATE INDEX idx_analyzed_content_raw ON analyzed_content(raw_content_id);
CREATE INDEX idx_analyzed_content_themes ON analyzed_content(key_themes);
CREATE INDEX idx_analyzed_content_tickers ON analyzed_content(tickers_mentioned);
```

### Table: confluence_scores
Stores pillar-by-pillar scores for analyzed content.

```sql
CREATE TABLE confluence_scores (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    analyzed_content_id INTEGER NOT NULL,
    
    -- Core 5 pillars (0-2 each)
    macro_score INTEGER NOT NULL,
    fundamentals_score INTEGER NOT NULL,
    valuation_score INTEGER NOT NULL,
    positioning_score INTEGER NOT NULL,
    policy_score INTEGER NOT NULL,
    
    -- Hybrid 2 pillars (0-2 each)
    price_action_score INTEGER NOT NULL,
    options_vol_score INTEGER NOT NULL,
    
    -- Totals
    core_total INTEGER NOT NULL,
    total_score INTEGER NOT NULL,
    meets_threshold BOOLEAN NOT NULL,
    
    -- Reasoning
    reasoning TEXT NOT NULL,
    falsification_criteria JSON,
    
    scored_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (analyzed_content_id) REFERENCES analyzed_content(id)
);

CREATE INDEX idx_confluence_scores_analyzed ON confluence_scores(analyzed_content_id);
CREATE INDEX idx_confluence_scores_total ON confluence_scores(total_score DESC);
CREATE INDEX idx_confluence_scores_threshold ON confluence_scores(meets_threshold);
```

### Table: themes
Tracks active investment ideas being monitored.

```sql
CREATE TABLE themes (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    name TEXT NOT NULL UNIQUE,  -- "Tech sector rotation", "Fed pivot imminent"
    description TEXT,
    
    -- Confluence tracking
    current_conviction REAL NOT NULL,  -- 0.0 to 1.0
    confidence_interval_low REAL,
    confidence_interval_high REAL,
    
    -- Theme lifecycle
    first_mentioned_at TIMESTAMP,
    status TEXT DEFAULT 'active',  -- "active", "acted_upon", "invalidated", "archived"
    
    -- Bayesian tracking
    prior_probability REAL,
    evidence_count INTEGER DEFAULT 0,
    
    metadata JSON,  -- Supporting sources, related tickers, etc.
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_themes_status ON themes(status);
CREATE INDEX idx_themes_conviction ON themes(current_conviction DESC);
```

### Table: theme_evidence
Links analyzed content to themes (many-to-many).

```sql
CREATE TABLE theme_evidence (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    theme_id INTEGER NOT NULL,
    analyzed_content_id INTEGER NOT NULL,
    supports_theme BOOLEAN DEFAULT 1,  -- False if contradicts
    evidence_strength REAL NOT NULL,  -- 0.0 to 1.0
    added_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (theme_id) REFERENCES themes(id),
    FOREIGN KEY (analyzed_content_id) REFERENCES analyzed_content(id),
    UNIQUE(theme_id, analyzed_content_id)
);

CREATE INDEX idx_theme_evidence_theme ON theme_evidence(theme_id);
CREATE INDEX idx_theme_evidence_content ON theme_evidence(analyzed_content_id);
```

### Table: bayesian_updates
Tracks how conviction changed over time for themes.

```sql
CREATE TABLE bayesian_updates (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    theme_id INTEGER NOT NULL,
    prior_conviction REAL NOT NULL,
    posterior_conviction REAL NOT NULL,
    evidence_analyzed_content_id INTEGER,
    update_reason TEXT,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (theme_id) REFERENCES themes(id),
    FOREIGN KEY (evidence_analyzed_content_id) REFERENCES analyzed_content(id)
);

CREATE INDEX idx_bayesian_updates_theme ON bayesian_updates(theme_id);
CREATE INDEX idx_bayesian_updates_time ON bayesian_updates(updated_at);
```

---

## Tasks

### Task 2.1: Create schema.sql
**Estimate**: 2 hours

Create the complete SQL schema file with all tables and indexes.

**Acceptance Criteria**:
- All 7 tables defined
- All indexes created
- Foreign key constraints in place
- Can execute schema.sql to create empty database

### Task 2.2: Build Database Manager
**Estimate**: 3 hours

Create `backend/utils/db.py` with database connection and utility functions.

```python
# Example structure
class DatabaseManager:
    def __init__(self, db_url):
        pass
    
    def get_connection(self):
        pass
    
    def execute_query(self, query, params):
        pass
    
    def insert(self, table, data):
        pass
    
    def update(self, table, id, data):
        pass
    
    def delete(self, table, id):
        pass
    
    def select(self, table, filters):
        pass
```

**Acceptance Criteria**:
- Connection pooling works
- CRUD operations functional
- Error handling robust
- Can execute raw SQL when needed

### Task 2.3: Create SQLAlchemy Models
**Estimate**: 4 hours

Create ORM models in `backend/models.py` for each table.

**Acceptance Criteria**:
- Models match schema exactly
- Relationships defined (ForeignKeys)
- Can perform queries using ORM
- Includes helpful methods (e.g., `Theme.add_evidence()`)

### Task 2.4: Build Migration System
**Estimate**: 2 hours

Simple migration system for schema changes.

```python
# database/migrations/001_initial_schema.py
def upgrade(db):
    # Apply changes
    pass

def downgrade(db):
    # Revert changes
    pass
```

**Acceptance Criteria**:
- Can apply migrations forward
- Can rollback migrations
- Tracks which migrations applied
- Works with empty database

### Task 2.5: Write Database Tests
**Estimate**: 3 hours

Create `tests/test_database.py` with comprehensive tests.

**Test Coverage**:
- Insert data into each table
- Query with filters
- Update records
- Delete records
- Foreign key constraints work
- Indexes improve query speed
- Migration up/down works

**Acceptance Criteria**:
- All tests pass
- 95%+ coverage of database code
- Tests run in <5 seconds

### Task 2.6: Seed Sample Data
**Estimate**: 2 hours

Create script to insert realistic sample data for development.

**Data**:
- 3-5 sources configured
- 50+ raw_content items (mix of types)
- 30+ analyzed_content items
- 20+ confluence_scores
- 5 themes with evidence
- Bayesian update history

**Acceptance Criteria**:
- Script runs without errors
- Can query sample data successfully
- Covers all table types
- Realistic enough for UI development

---

## Testing Plan

### Unit Tests
- Test each CRUD operation
- Test foreign key constraints
- Test indexes (query performance)
- Test migration system

### Integration Tests
- Full pipeline: insert → analyze → score → theme
- Concurrent access (multiple writers)
- Large dataset performance (1000+ records)

### Manual Testing
- Sebastian reviews schema design
- Query sample data from command line
- Verify indexes with EXPLAIN QUERY PLAN

---

## Performance Requirements

- Common queries: <100ms
- Complex joins (theme evidence): <500ms
- Batch inserts (100 items): <2 seconds
- Full database backup: <5 seconds

---

## Risks & Mitigations

| Risk | Impact | Mitigation |
|------|--------|------------|
| Schema changes needed later | Medium | Migration system handles this |
| SQLite performance insufficient | High | Plan Postgres migration path, but unlikely for this data volume |
| JSON fields too flexible | Low | Document expected structure, validate in code |

---

## Deliverables

1. `database/schema.sql` - Complete schema
2. `backend/utils/db.py` - Database manager
3. `backend/models.py` - SQLAlchemy models
4. `database/migrations/001_initial_schema.py` - First migration
5. `tests/test_database.py` - Comprehensive tests
6. `scripts/seed_data.py` - Sample data script
7. Documentation of schema in this PRD

---

**Status**: Ready to begin after PRD-001 completion
