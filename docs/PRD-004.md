# PRD-004: Basic Collectors (No AI)

**Version**: 1.0  
**Date**: 2025-11-18  
**Phase**: 1 - Foundation  
**Duration**: 3 days  
**Status**: Not Started  
**Dependencies**: PRD-002

---

## Objective

Build raw data collection modules for all 6 sources. Collectors fetch content and save to database without AI analysis (that comes later). Focus on reliability and error handling.

---

## Success Criteria

- [ ] All 6 collectors functional
- [ ] Handles authentication for 42macro
- [ ] Discord local script works on Sebastian's laptop
- [ ] Twitter scraping bypasses rate limits
- [ ] YouTube API integration working
- [ ] RSS parsing reliable
- [ ] Error handling and retry logic
- [ ] Data saved to raw_content table

---

## Collector 1: 42macro

### Authentication
- Login with email/password
- Handle session management
- Detect if re-login needed

### Content to Collect
1. **PDFs**: "Leadoff Morning Note", "Around The Horn", "Macro Scouting Report"
2. **Videos**: "Macro Minute" daily videos
3. **KISS Model**: Portfolio signals and Excel downloads
4. **Community Posts**: Text discussions

### Implementation
```python
class Macro42Collector:
    def __init__(self, email, password):
        self.session = requests.Session()
        self.email = email
        self.password = password
    
    def login(self):
        """Authenticate with 42macro."""
        pass
    
    def collect_pdfs(self, start_date, end_date):
        """Download PDF research reports."""
        pass
    
    def collect_videos(self, start_date, end_date):
        """Get video URLs and metadata."""
        pass
    
    def save_to_database(self, content):
        """Save to raw_content table."""
        pass
```

### Challenges
- PDF URLs are S3 signed URLs (may expire)
- Need to download and store locally
- Video links may be private

---

## Collector 2: Discord (Local Script)

### Authentication
Uses Sebastian's logged-in Discord session via `discord.py-self`.

### Content to Collect
From channels:
- `stocks-chat`
- `crypto-weekly`
- `macro-daily`
- `spx-fixed-strike-vol`
- `vix-monitor`

### Data Types
- Text messages
- Attached images (charts)
- Attached PDFs
- Zoom/Webex video links (don't download video, just get link)

### Implementation
```python
import discord_self

class DiscordSelfCollector:
    def __init__(self, user_token):
        self.client = discord_self.Client()
        self.token = user_token
    
    async def collect_channel(self, channel_id, start_time):
        """Fetch messages from a channel since start_time."""
        pass
    
    async def download_attachments(self, message):
        """Download PDFs and images."""
        pass
    
    def extract_video_links(self, message):
        """Extract Zoom/Webex URLs."""
        pass
```

### Scheduling
- Runs at 6am, 6pm via Windows Task Scheduler
- If laptop off, runs on next startup
- POSTs data to Railway API endpoint `/ingest/discord`

---

## Collector 3: Twitter Scraper

### Accounts to Monitor
- `@KTTECHPRIVATE`
- `@MelMattison1`

### Content to Collect
- Tweet text
- Tweet images
- Video links
- Thread continuations

### Implementation (no API)
Uses web scraping (Selenium or requests + BeautifulSoup):

```python
class TwitterScraper:
    def __init__(self, session_token):
        self.session = requests.Session()
        self.session_token = session_token
    
    def get_user_tweets(self, username, count=20):
        """Scrape recent tweets from a user."""
        pass
    
    def download_media(self, tweet):
        """Download images/videos from tweet."""
        pass
```

### Challenges
- Twitter heavily rate-limits scrapers
- May need to rotate IPs or use session tokens
- Tweet threads need to be linked

---

## Collector 4: YouTube API

### Channels to Monitor
- Peter Diamandis
- Jordi Visser Labs
- Forward Guidance
- 42 Macro

### Content to Collect
- Video metadata (title, description, published date)
- Automatic captions/transcripts (if available)
- Thumbnail images

### Implementation
```python
from googleapiclient.discovery import build

class YouTubeCollector:
    def __init__(self, api_key):
        self.youtube = build('youtube', 'v3', developerKey=api_key)
    
    def get_channel_videos(self, channel_id, max_results=10):
        """Fetch recent videos from a channel."""
        pass
    
    def get_video_details(self, video_id):
        """Get video metadata."""
        pass
    
    def get_transcript(self, video_id):
        """Get automatic captions if available."""
        pass
```

---

## Collector 5: Substack RSS

### Account
- Visser Labs: https://visserlabs.substack.com/

### Content to Collect
- Article title, author, published date
- Full article text
- Any embedded images

### Implementation
```python
import feedparser

class SubstackCollector:
    def __init__(self, feed_url):
        self.feed_url = feed_url
    
    def fetch_posts(self, count=10):
        """Parse RSS feed for recent posts."""
        pass
    
    def get_full_article(self, post_url):
        """Scrape full article content."""
        pass
```

---

## Tasks

### Task 4.1: Build Base Collector Class
**Estimate**: 2 hours

Create abstract base class for all collectors.

### Task 4.2: Implement Macro42Collector
**Estimate**: 8 hours

Authentication, PDF downloads, video links.

### Task 4.3: Implement DiscordSelfCollector
**Estimate**: 6 hours

Local script using discord.py-self, save data to Railway API.

### Task 4.4: Implement TwitterScraper
**Estimate**: 6 hours

Web scraping without API, handle rate limits.

### Task 4.5: Implement YouTubeCollector
**Estimate**: 4 hours

Use YouTube Data API v3.

### Task 4.6: Implement SubstackCollector
**Estimate**: 3 hours

RSS parsing and full article scraping.

### Task 4.7: Create Collection Orchestrator
**Estimate**: 3 hours

Script that runs all collectors on schedule or on-demand.

### Task 4.8: Write Collector Tests
**Estimate**: 4 hours

Unit tests for each collector.

---

## Testing Plan

### Manual Testing
Sebastian will:
- Run each collector standalone
- Verify data appears in database
- Check file downloads are correct
- Confirm Discord script works on his laptop

### Automated Testing
- Mock HTTP responses
- Test error handling (network failures, auth errors)
- Test retry logic

---

## Error Handling

All collectors must handle:
- Network timeouts
- Authentication failures
- Rate limits (exponential backoff)
- Malformed content
- Disk space issues (for file downloads)

---

## Deliverables

1. `collectors/base_collector.py` - Abstract base class
2. `collectors/macro42.py` - 42macro collector
3. `collectors/discord_self.py` - Discord local script
4. `collectors/twitter_scraper.py` - Twitter scraper
5. `collectors/youtube_api.py` - YouTube collector
6. `collectors/substack_rss.py` - Substack collector
7. `scripts/run_collectors.py` - Orchestration script
8. Tests for all collectors

---

**Status**: Ready after PRD-002 completion
