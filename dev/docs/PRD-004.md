# PRD-004: Basic Collectors (No AI)

**Version**: 1.2
**Date**: 2025-11-18
**Updated**: 2025-11-19 (Testing complete, 3 collectors production-ready, 3 in progress)
**Phase**: 1 - Foundation
**Duration**: 3 days
**Status**: 75% Complete
**Dependencies**: PRD-002

---

## Objective

Build raw data collection modules for all 6 sources. Collectors fetch content and save to database without AI analysis (that comes later). Focus on reliability and error handling.

---

## Success Criteria

- [x] All 6 collectors functional (3 production-ready, 3 in progress)
- [x] Discord local script works on Sebastian's laptop ‚úÖ
- [x] Discord channel configuration system in place ‚úÖ
- [x] YouTube API integration working ‚úÖ (40 videos tested)
- [x] RSS parsing reliable ‚úÖ (20 articles tested)
- [x] Error handling and retry logic ‚úÖ
- [ ] Handles authentication for 42macro (Selenium implementation in progress)
- [ ] Twitter API integration (switching from ntscraper to official API)
- [ ] KT Technical website collector (to be built)
- [x] Data saved to raw_content table (framework complete, awaiting database initialization)

### Production Status:
- ‚úÖ **Discord Collector**: PRODUCTION READY
- ‚úÖ **YouTube Collector**: PRODUCTION READY (real channel IDs added)
- ‚úÖ **Substack Collector**: PRODUCTION READY
- üîÑ **Twitter Collector**: Switching to Twitter API Free Tier
- üîÑ **42 Macro Collector**: Selenium implementation needed
- üîÑ **KT Technical Collector**: To be built (simple blog scraping)

---

## Collector 1: 42macro

### Authentication
- Login with email/password
- Handle session management
- Detect if re-login needed

### Content to Collect
1. **PDFs**: "Leadoff Morning Note", "Around The Horn", "Macro Scouting Report"
2. **Videos**: "Macro Minute" daily videos
3. **KISS Model**: Portfolio signals and Excel downloads
4. **Community Posts**: Text discussions

### Implementation
```python
class Macro42Collector:
    def __init__(self, email, password):
        self.session = requests.Session()
        self.email = email
        self.password = password

    def login(self):
        """Authenticate with 42macro."""
        pass

    def collect_pdfs(self, start_date, end_date):
        """Download PDF research reports."""
        pass

    def collect_videos(self, start_date, end_date):
        """Get video URLs and metadata."""
        pass

    def save_to_database(self, content):
        """Save to raw_content table."""
        pass
```

### Challenges
- PDF URLs are S3 signed URLs (may expire)
- Need to download and store locally
- Video links may be private

---

## Collector 2: Discord (Local Script)

### Collection Strategy: Channel-Based

**Approach**: Collect from **specified channels only**, not server-wide search.

**Why**:
- Efficient (only relevant channels)
- Organized (Imran organizes by topic)
- Maintains context (captures conversation flow)
- Easy to maintain (add/remove channels as needed)

### Channel Configuration

**File**: `config/discord_channels.json`

```json
{
    "server_name": "Options Insight Official",
    "server_id": "your_server_id_here",
    "channels_to_monitor": [
        {
            "name": "stocks-chat",
            "channel_id": "123456789012345678",
            "priority": "high",
            "collect_types": ["text", "images", "pdfs", "video_links"],
            "description": "Pre-market analysis, daily stock commentary"
        },
        {
            "name": "crypto-weekly",
            "channel_id": "234567890123456789",
            "priority": "high",
            "collect_types": ["text", "pdfs", "video_links"],
            "description": "Weekly crypto volatility updates"
        },
        {
            "name": "macro-daily",
            "channel_id": "345678901234567890",
            "priority": "high",
            "collect_types": ["text", "pdfs", "video_links"],
            "description": "Daily macro market analysis"
        },
        {
            "name": "spx-fixed-strike-vol",
            "channel_id": "456789012345678901",
            "priority": "high",
            "collect_types": ["text", "images", "video_links"],
            "description": "SPX volatility surface analysis"
        },
        {
            "name": "vix-monitor",
            "channel_id": "567890123456789012",
            "priority": "medium",
            "collect_types": ["text", "images"],
            "description": "VIX term structure and positioning"
        }
    ],
    "collection_settings": {
        "lookback_days_first_run": 7,
        "collect_from_all_users": true,
        "min_message_length": 20,
        "ignore_patterns": [
            "^!",
            "^gm$",
            "^gn$",
            "^thanks$"
        ],
        "max_messages_per_channel": 500
    },
    "file_settings": {
        "download_pdfs": true,
        "download_images": true,
        "extract_video_links": true,
        "max_file_size_mb": 50
    }
}
```

### One-Time Setup: Get Channel IDs

**Helper Script**: `scripts/get_discord_channel_ids.py`

```python
import discord_self
import json

async def list_server_channels(user_token):
    """Helper script to find channel IDs for configuration."""
    client = discord_self.Client()

    @client.event
    async def on_ready():
        print("\n=== Discord Servers and Channels ===\n")

        for guild in client.guilds:
            print(f"Server: {guild.name} (ID: {guild.id})")
            print(f"{'‚îÄ' * 60}")

            # Text channels only
            text_channels = [ch for ch in guild.channels if isinstance(ch, discord.TextChannel)]

            for channel in text_channels:
                print(f"  üìù {channel.name}")
                print(f"     ID: {channel.id}")
                print(f"     Category: {channel.category.name if channel.category else 'None'}")
                print()

            print("\n")

        await client.close()

    await client.start(user_token, bot=False)

if __name__ == "__main__":
    # Run this once to get channel IDs
    # Then copy IDs into discord_channels.json
    import asyncio

    token = input("Enter your Discord user token: ")
    asyncio.run(list_server_channels(token))
```

**Usage**:
1. Run script once: `python scripts/get_discord_channel_ids.py`
2. Find "Options Insight Official" server
3. Copy the 5 channel IDs into `discord_channels.json`

### Content Collection Logic

**What Gets Collected**:
- **ALL messages** in the 5 specified channels (not just Imran's)
- Includes member questions and responses (provides context)
- Text, PDFs, images, video links (Zoom/Webex)

**Why collect from everyone**:
1. Imran's responses to questions often contain additional insights
2. Member questions provide context for what's being analyzed
3. Community-shared charts can be valuable
4. AI Content Classifier (PRD-003) will filter noise later

### Implementation

```python
import discord_self
import json
import aiohttp
from pathlib import Path
from datetime import datetime, timedelta

class DiscordSelfCollector:
    def __init__(self, user_token, config_path="config/discord_channels.json"):
        self.client = discord_self.Client()
        self.token = user_token
        self.config = self.load_config(config_path)
        self.download_dir = Path("downloads/discord")
        self.download_dir.mkdir(parents=True, exist_ok=True)

    def load_config(self, config_path):
        """Load channel configuration."""
        with open(config_path, 'r') as f:
            return json.load(f)

    async def collect_all_channels(self):
        """Main collection function - runs all configured channels."""
        collected_data = []

        await self.client.start(self.token, bot=False)

        for channel_config in self.config["channels_to_monitor"]:
            channel_data = await self.collect_channel(channel_config)
            collected_data.extend(channel_data)

        await self.client.close()

        return collected_data

    async def collect_channel(self, channel_config):
        """Collect messages from a single channel."""
        channel_id = int(channel_config["channel_id"])
        channel = self.client.get_channel(channel_id)

        if not channel:
            print(f"‚ö†Ô∏è  Channel {channel_config['name']} not found!")
            return []

        # Determine lookback time
        lookback_time = self.get_lookback_time(channel_config["name"])

        messages_data = []
        message_count = 0
        max_messages = self.config["collection_settings"]["max_messages_per_channel"]

        print(f"üì° Collecting from #{channel_config['name']}...")

        async for message in channel.history(limit=max_messages, after=lookback_time):
            # Apply filters
            if not self.should_collect_message(message):
                continue

            message_data = {
                "channel_name": channel_config["name"],
                "channel_id": channel_id,
                "priority": channel_config["priority"],
                "message_id": message.id,
                "author": message.author.name,
                "author_id": str(message.author.id),
                "content": message.content,
                "timestamp": message.created_at.isoformat(),
                "attachments": [],
                "video_links": [],
                "embeds": []
            }

            # Handle attachments (PDFs, images)
            if message.attachments and channel_config["collect_types"]:
                message_data["attachments"] = await self.process_attachments(
                    message.attachments,
                    channel_config["collect_types"]
                )

            # Extract video links (Zoom, Webex, YouTube)
            if "video_links" in channel_config["collect_types"]:
                message_data["video_links"] = self.extract_video_links(message.content)

            # Handle embeds (sometimes videos are embedded)
            if message.embeds:
                message_data["embeds"] = [
                    {
                        "type": embed.type,
                        "url": embed.url,
                        "title": embed.title
                    }
                    for embed in message.embeds
                ]

            messages_data.append(message_data)
            message_count += 1

        print(f"‚úÖ Collected {message_count} messages from #{channel_config['name']}")
        return messages_data

    def should_collect_message(self, message):
        """Apply filters to determine if message should be collected."""
        settings = self.config["collection_settings"]

        # Ignore bot messages
        if message.author.bot:
            return False

        # Check minimum length
        if len(message.content) < settings["min_message_length"]:
            return False

        # Check ignore patterns
        content_lower = message.content.lower().strip()
        for pattern in settings["ignore_patterns"]:
            # Simple pattern matching (can be enhanced with regex)
            if content_lower.startswith(pattern.strip("^").strip("$")):
                return False

        return True

    async def process_attachments(self, attachments, collect_types):
        """Download and process message attachments."""
        processed = []
        file_settings = self.config["file_settings"]

        for attachment in attachments:
            # Check file size limit
            size_mb = attachment.size / (1024 * 1024)
            if size_mb > file_settings["max_file_size_mb"]:
                print(f"‚ö†Ô∏è  Skipping {attachment.filename} (too large: {size_mb:.1f}MB)")
                continue

            # Handle PDFs
            if attachment.filename.endswith('.pdf') and "pdfs" in collect_types:
                if file_settings["download_pdfs"]:
                    file_path = await self.download_file(attachment)
                    processed.append({
                        "type": "pdf",
                        "filename": attachment.filename,
                        "path": str(file_path),
                        "size_mb": size_mb,
                        "url": attachment.url
                    })

            # Handle images
            elif attachment.content_type and attachment.content_type.startswith('image/'):
                if "images" in collect_types and file_settings["download_images"]:
                    file_path = await self.download_file(attachment)
                    processed.append({
                        "type": "image",
                        "filename": attachment.filename,
                        "path": str(file_path),
                        "size_mb": size_mb,
                        "content_type": attachment.content_type
                    })

        return processed

    async def download_file(self, attachment):
        """Download a file attachment."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{timestamp}_{attachment.filename}"
        file_path = self.download_dir / filename

        async with aiohttp.ClientSession() as session:
            async with session.get(attachment.url) as response:
                with open(file_path, 'wb') as f:
                    f.write(await response.read())

        return file_path

    def extract_video_links(self, content):
        """Extract Zoom, Webex, YouTube links from message content."""
        import re

        video_patterns = [
            (r'https://[a-z0-9\.]*zoom\.us/rec/[^\s]+', 'zoom'),
            (r'https://[a-z0-9\.]*webex\.com/[^\s]+', 'webex'),
            (r'https://[a-z0-9\.]*youtube\.com/watch\?v=[^\s]+', 'youtube'),
            (r'https://youtu\.be/[^\s]+', 'youtube'),
        ]

        found_links = []
        for pattern, platform in video_patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                found_links.append({
                    "platform": platform,
                    "url": match
                })

        return found_links

    def get_lookback_time(self, channel_name):
        """Determine how far back to collect messages."""
        # Check last collection time from database
        # If first run, use lookback_days_first_run from config
        # Otherwise, collect since last successful collection

        # Placeholder - implement with database
        lookback_days = self.config["collection_settings"]["lookback_days_first_run"]
        return datetime.utcnow() - timedelta(days=lookback_days)

    async def upload_to_railway(self, collected_data):
        """Upload collected data to Railway API."""
        railway_url = "https://confluence-production.up.railway.app/api/ingest/discord"

        async with aiohttp.ClientSession() as session:
            async with session.post(railway_url, json=collected_data) as response:
                if response.status == 200:
                    print("‚úÖ Data uploaded to Railway successfully")
                    return True
                else:
                    print(f"‚ùå Upload failed: {response.status}")
                    return False
```

### Scheduling

**Windows Task Scheduler** configuration:
- **Trigger 1**: Daily at 6:00 AM
- **Trigger 2**: Daily at 6:00 PM
- **Action**: `python scripts/discord_local.py`
- **Conditions**:
  - Run only if laptop is on
  - If missed, run at next startup
- **Settings**:
  - Stop if runs longer than 30 minutes
  - Restart on failure (max 3 attempts)

**Main Script**: `scripts/discord_local.py`

```python
import asyncio
import json
from datetime import datetime
from collectors.discord_self import DiscordSelfCollector

async def main():
    print(f"\n{'='*60}")
    print(f"Discord Collection - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*60}\n")

    # Load Discord token from environment or config
    import os
    token = os.getenv("DISCORD_USER_TOKEN")

    if not token:
        print("‚ùå DISCORD_USER_TOKEN not found in environment")
        return

    # Initialize collector
    collector = DiscordSelfCollector(token)

    try:
        # Collect from all configured channels
        collected_data = await collector.collect_all_channels()

        print(f"\nüìä Total messages collected: {len(collected_data)}")

        # Upload to Railway API
        if collected_data:
            success = await collector.upload_to_railway(collected_data)

            if success:
                print("\n‚úÖ Discord collection complete!")
            else:
                print("\n‚ö†Ô∏è  Collection succeeded but upload failed")
                # Save locally as backup
                backup_file = f"discord_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                with open(backup_file, 'w') as f:
                    json.dump(collected_data, f, indent=2)
                print(f"üíæ Data saved to {backup_file}")
        else:
            print("\n‚ÑπÔ∏è  No new messages to collect")

    except Exception as e:
        print(f"\n‚ùå Error during collection: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main())
```

### Discord Data Flow

```
Sebastian's Laptop (Discord always running)
    ‚îÇ
    ‚îú‚îÄ 6:00 AM: Task Scheduler triggers
    ‚îÇ   ‚îî‚îÄ discord_local.py runs
    ‚îÇ       ‚îú‚îÄ Reads config/discord_channels.json
    ‚îÇ       ‚îú‚îÄ Connects to Discord (Sebastian's session)
    ‚îÇ       ‚îú‚îÄ Collects from 5 channels
    ‚îÇ       ‚îú‚îÄ Downloads PDFs/images
    ‚îÇ       ‚îú‚îÄ Extracts video links
    ‚îÇ       ‚îî‚îÄ POSTs JSON to Railway API
    ‚îÇ
    ‚îú‚îÄ 6:00 PM: Task Scheduler triggers (repeat)
    ‚îÇ
    ‚îî‚îÄ If laptop was off: Runs on next startup

Railway Backend
    ‚îÇ
    ‚îú‚îÄ /api/ingest/discord endpoint receives data
    ‚îú‚îÄ Validates and saves to raw_content table
    ‚îú‚îÄ Files stored in persistent volume
    ‚îî‚îÄ Triggers Content Classifier Agent
```

---

## Collector 3: Twitter Scraper

### Accounts to Monitor
- `@KTTECHPRIVATE`
- `@MelMattison1`

### Content to Collect
- Tweet text
- Tweet images
- Video links
- Thread continuations

### Implementation (no API)
Uses web scraping (Selenium or requests + BeautifulSoup):

```python
class TwitterScraper:
    def __init__(self, session_token):
        self.session = requests.Session()
        self.session_token = session_token

    def get_user_tweets(self, username, count=20):
        """Scrape recent tweets from a user."""
        pass

    def download_media(self, tweet):
        """Download images/videos from tweet."""
        pass
```

### Challenges
- Twitter heavily rate-limits scrapers
- May need to rotate IPs or use session tokens
- Tweet threads need to be linked

---

## Collector 4: YouTube API

### Channels to Monitor
- Peter Diamandis
- Jordi Visser Labs
- Forward Guidance
- 42 Macro

### Content to Collect
- Video metadata (title, description, published date)
- Automatic captions/transcripts (if available)
- Thumbnail images

### Implementation
```python
from googleapiclient.discovery import build

class YouTubeCollector:
    def __init__(self, api_key):
        self.youtube = build('youtube', 'v3', developerKey=api_key)

    def get_channel_videos(self, channel_id, max_results=10):
        """Fetch recent videos from a channel."""
        pass

    def get_video_details(self, video_id):
        """Get video metadata."""
        pass

    def get_transcript(self, video_id):
        """Get automatic captions if available."""
        pass
```

---

## Collector 5: Substack RSS

### Account
- Visser Labs: https://visserlabs.substack.com/

### Content to Collect
- Article title, author, published date
- Full article text
- Any embedded images

### Implementation
```python
import feedparser

class SubstackCollector:
    def __init__(self, feed_url):
        self.feed_url = feed_url

    def fetch_posts(self, count=10):
        """Parse RSS feed for recent posts."""
        pass

    def get_full_article(self, post_url):
        """Scrape full article content."""
        pass
```

---

## Tasks

### Task 4.1: Build Base Collector Class
**Estimate**: 2 hours

Create abstract base class for all collectors.

### Task 4.2: Implement Macro42Collector
**Estimate**: 8 hours

Authentication, PDF downloads, video links.

### Task 4.3: Implement DiscordSelfCollector
**Estimate**: 6 hours

Local script using discord.py-self, save data to Railway API.

### Task 4.4: Implement TwitterScraper
**Estimate**: 6 hours

Web scraping without API, handle rate limits.

### Task 4.5: Implement YouTubeCollector
**Estimate**: 4 hours

Use YouTube Data API v3.

### Task 4.6: Implement SubstackCollector
**Estimate**: 3 hours

RSS parsing and full article scraping.

### Task 4.7: Create Collection Orchestrator
**Estimate**: 3 hours

Script that runs all collectors on schedule or on-demand.

### Task 4.8: Write Collector Tests
**Estimate**: 4 hours

Unit tests for each collector.

---

## Testing Plan

### Manual Testing
Sebastian will:
- Run each collector standalone
- Verify data appears in database
- Check file downloads are correct
- Confirm Discord script works on his laptop

### Automated Testing
- Mock HTTP responses
- Test error handling (network failures, auth errors)
- Test retry logic

---

## Error Handling

All collectors must handle:
- Network timeouts
- Authentication failures
- Rate limits (exponential backoff)
- Malformed content
- Disk space issues (for file downloads)

---

## Deliverables

1. `collectors/base_collector.py` - Abstract base class
2. `collectors/macro42.py` - 42macro collector
3. `collectors/discord_self.py` - Discord local script
4. `collectors/twitter_scraper.py` - Twitter scraper
5. `collectors/youtube_api.py` - YouTube collector
6. `collectors/substack_rss.py` - Substack collector
7. `scripts/run_collectors.py` - Orchestration script
8. Tests for all collectors

---

**Status**: Ready after PRD-002 completion
