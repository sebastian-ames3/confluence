# PRD-019: Duplicate Detection for All Collectors

**Version**: 1.0
**Date**: 2025-12-03
**Phase**: 6 - Production Hardening
**Status**: Not Started

---

## Objective

Implement consistent duplicate detection across all data collection paths to prevent the same content from being stored multiple times in the database. Currently, most API endpoints and collectors do not check for existing content before saving.

**Priority**: P1 - High (Data Integrity)

---

## Background

Analysis revealed that duplicate detection is **inconsistent** across the codebase:

| Component | Has Duplicate Check | Status |
|-----------|---------------------|--------|
| `BaseCollector.save_to_database()` | Partial (message_id or url) | Exists but limited |
| `/api/collect/discord` endpoint | None | **Critical** |
| `/api/collect/42macro` endpoint | None | **Critical** |
| `/api/trigger/{source}` endpoint | None | **Critical** |
| `YouTubeCollector` | None | **High** |
| `SubstackCollector` | None | **High** |
| `Macro42Collector` (PDFs) | None | **High** |
| `Macro42Collector` (Videos) | Local set only | Partial |
| `KTTechnicalCollector` | None | Medium |

### Impact
- Running collection twice creates duplicate records
- Scheduled tasks will accumulate duplicates daily
- Database bloat and incorrect content counts
- Synthesis may process same content multiple times

---

## Success Criteria

- [ ] All API ingestion endpoints check for duplicates before saving
- [ ] All collectors check database for existing content
- [ ] Unique identifiers defined per source type
- [ ] Duplicate skip count logged for monitoring
- [ ] Database indexes added for efficient lookups
- [ ] Existing duplicates cleaned up (migration)
- [ ] Tests verify duplicate prevention

---

## Technical Specifications

### 1. Unique Identifiers by Source

| Source | Primary Key | Field Location | Fallback |
|--------|-------------|----------------|----------|
| Discord | `message_id` | `metadata.message_id` | None |
| YouTube | `video_id` | `metadata.video_id` | `url` |
| Substack | `url` | `url` field | None |
| 42 Macro PDFs | `report_type + date` | `metadata.report_type` + `metadata.date` | `url` |
| 42 Macro Videos | `video_id` | `metadata.video_id` | `url` |
| KT Technical | `url` | `url` field | None |

### 2. Database Schema Changes

**File**: `backend/models.py`

Add indexes for efficient duplicate lookups:

```python
class RawContent(Base):
    __tablename__ = "raw_content"

    # Existing columns...

    __table_args__ = (
        Index('idx_source_url', 'source_id', 'url'),
        Index('idx_source_content_type', 'source_id', 'content_type'),
    )
```

### 3. Utility Function

**File**: `backend/utils/deduplication.py` (new)

```python
from sqlalchemy.orm import Session
from backend.models import RawContent
import logging

logger = logging.getLogger(__name__)

def check_duplicate(
    db: Session,
    source_id: int,
    url: str = None,
    message_id: str = None,
    video_id: str = None,
    report_type: str = None,
    date: str = None
) -> bool:
    """
    Check if content already exists in database.

    Returns True if duplicate found, False if new content.
    """
    # Check by URL (most common)
    if url:
        existing = db.query(RawContent).filter(
            RawContent.source_id == source_id,
            RawContent.url == url
        ).first()
        if existing:
            return True

    # Check by message_id (Discord)
    if message_id:
        existing = db.query(RawContent).filter(
            RawContent.source_id == source_id,
            RawContent.json_metadata.contains(f'"message_id": "{message_id}"')
        ).first()
        if existing:
            return True

    # Check by video_id (YouTube, Vimeo)
    if video_id:
        existing = db.query(RawContent).filter(
            RawContent.source_id == source_id,
            RawContent.json_metadata.contains(f'"video_id": "{video_id}"')
        ).first()
        if existing:
            return True

    # Check by report_type + date (42 Macro PDFs)
    if report_type and date:
        existing = db.query(RawContent).filter(
            RawContent.source_id == source_id,
            RawContent.json_metadata.contains(f'"report_type": "{report_type}"'),
            RawContent.json_metadata.contains(f'"date": "{date}"')
        ).first()
        if existing:
            return True

    return False
```

### 4. API Endpoint Updates

#### `/api/collect/discord` - `backend/routes/collect.py`

**Before** (line 84-96):
```python
raw_content = RawContent(...)
db.add(raw_content)
saved_count += 1
```

**After**:
```python
from backend.utils.deduplication import check_duplicate

# Inside the for loop, before creating RawContent:
message_id = message_data.get("metadata", {}).get("message_id")
if check_duplicate(db, source.id, message_id=message_id):
    skipped_duplicates += 1
    logger.debug(f"Skipping duplicate message: {message_id}")
    continue

raw_content = RawContent(...)
db.add(raw_content)
saved_count += 1
```

#### `/api/collect/42macro` - `backend/routes/collect.py`

**Add before RawContent creation** (line ~194):
```python
from backend.utils.deduplication import check_duplicate

# Inside the for loop:
metadata = item_data.get("metadata", {})
url = item_data.get("url")
video_id = metadata.get("video_id")
report_type = metadata.get("report_type")
date = metadata.get("date")

if check_duplicate(
    db, source.id,
    url=url,
    video_id=video_id,
    report_type=report_type,
    date=date
):
    skipped_duplicates += 1
    logger.debug(f"Skipping duplicate 42macro item: {url or video_id or report_type}")
    continue
```

#### `_save_collected_items()` - `backend/routes/trigger.py`

**Add duplicate check** (line ~389):
```python
from backend.utils.deduplication import check_duplicate

# Inside the for loop:
url = item.get("url")
metadata = item.get("metadata", {})
video_id = metadata.get("video_id")

if check_duplicate(db, source.id, url=url, video_id=video_id):
    skipped_duplicates += 1
    continue
```

### 5. Collector Updates

#### `YouTubeCollector` - `collectors/youtube_api.py`

Add database check in `_collect_channel_videos()`:
```python
# After extracting video_id, before appending:
# Note: Collectors don't have db session, so dedup happens at save time
# The collector should still track local duplicates within the run
collected_video_ids = set()

for item in playlist_response.get('items', []):
    video_id = item['contentDetails']['videoId']
    if video_id in collected_video_ids:
        continue
    collected_video_ids.add(video_id)
    # ... rest of processing
```

#### `SubstackCollector` - `collectors/substack_rss.py`

Add URL tracking in `_collect_feed()`:
```python
collected_urls = set()

for entry in feed.entries[:max_articles]:
    link = entry.get('link', '')
    if link in collected_urls:
        continue
    collected_urls.add(link)
    # ... rest of processing
```

#### `Macro42Collector` - `collectors/macro42_selenium.py`

PDFs already collect locally; add URL tracking:
```python
# In _collect_pdfs():
collected_titles = set()

# Before downloading:
title = f"{report_type} - {date_text}"
if title in collected_titles:
    continue
collected_titles.add(title)
```

### 6. Database Migration

**File**: `backend/migrations/add_dedup_indexes.py` (new)

```python
"""Add indexes for duplicate detection."""

from sqlalchemy import text

def upgrade(db_engine):
    with db_engine.connect() as conn:
        # Add index on (source_id, url) for fast URL lookups
        conn.execute(text("""
            CREATE INDEX IF NOT EXISTS idx_source_url
            ON raw_content (source_id, url)
        """))

        # Add index on (source_id, content_type) for filtering
        conn.execute(text("""
            CREATE INDEX IF NOT EXISTS idx_source_content_type
            ON raw_content (source_id, content_type)
        """))

        conn.commit()

def cleanup_existing_duplicates(db_engine):
    """Remove existing duplicate records, keeping the oldest."""
    with db_engine.connect() as conn:
        # Find and delete duplicates by URL
        conn.execute(text("""
            DELETE FROM raw_content
            WHERE id NOT IN (
                SELECT MIN(id)
                FROM raw_content
                WHERE url IS NOT NULL
                GROUP BY source_id, url
            )
            AND url IS NOT NULL
        """))

        conn.commit()
```

---

## Implementation Steps

### Phase 1: Core Infrastructure
1. Create `backend/utils/deduplication.py` with `check_duplicate()` function
2. Add database indexes via migration
3. Update `/api/collect/discord` endpoint
4. Update `/api/collect/42macro` endpoint
5. Test with existing data

### Phase 2: Trigger Endpoints
6. Update `_save_collected_items()` in trigger.py
7. Verify YouTube collection deduplication
8. Verify Substack collection deduplication

### Phase 3: Local Collectors
9. Add local dedup tracking to YouTubeCollector
10. Add local dedup tracking to SubstackCollector
11. Add local dedup tracking to Macro42Collector PDFs

### Phase 4: Cleanup & Testing
12. Run migration to clean existing duplicates
13. Add unit tests for deduplication
14. Update logging to report skip counts
15. Document deduplication behavior

---

## Response Format Updates

All collection endpoints should return skip counts:

```json
{
    "status": "success",
    "message": "Ingested 5 items",
    "saved": 5,
    "received": 9,
    "skipped_duplicates": 4,
    "total_in_db": 27,
    "timestamp": "2025-12-03T16:12:15.696552"
}
```

---

## Testing Strategy

### Unit Tests
```python
def test_duplicate_discord_message_skipped():
    """Same message_id should not be saved twice."""

def test_duplicate_url_skipped():
    """Same URL should not be saved twice for same source."""

def test_duplicate_video_id_skipped():
    """Same video_id should not be saved twice."""

def test_different_sources_same_url_allowed():
    """Same URL from different sources should both be saved."""
```

### Integration Tests
```python
def test_discord_endpoint_deduplication():
    """POST same messages twice, verify only saved once."""

def test_42macro_endpoint_deduplication():
    """POST same items twice, verify only saved once."""
```

---

## Risks & Mitigations

| Risk | Mitigation |
|------|------------|
| JSON contains() is slow | Add database indexes; consider dedicated columns |
| Existing duplicates in DB | Run cleanup migration before enabling checks |
| False positives | Use multiple fields for compound keys |
| Performance impact | Index on (source_id, url) for O(log n) lookups |

---

## Out of Scope

- Content hash-based deduplication (comparing actual text)
- Cross-source deduplication (same article from different sources)
- Update detection (same ID but modified content)
- Dedicated ID columns (would require schema migration)

---

## Dependencies

- SQLAlchemy for database queries
- Existing RawContent model
- No new external packages required

---

## Estimated Effort

| Task | Estimate |
|------|----------|
| Deduplication utility | 30 min |
| API endpoint updates | 45 min |
| Collector updates | 30 min |
| Database migration | 20 min |
| Testing | 45 min |
| **Total** | ~3 hours |

---

## References

- Analysis: Comprehensive duplicate detection analysis (Dec 3, 2025)
- Related: PRD-017 (Polish & Reliability)
- Files: `backend/routes/collect.py`, `backend/routes/trigger.py`, `collectors/*.py`
